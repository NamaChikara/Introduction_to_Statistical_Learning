---
title: "Chapter 7 Lab"
author: "ZackBarry"
date: "7/26/2019"
output: 
  html_document:
  toc: true
toc_depth: 2
number_sections: true
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(dplyr)
library(tibble)
library(ggplot2)
library(MASS) # for lda()
library(class) # for knn()
library(boot) # for cv.glm() and boot()
library(MVN)  # for checking for multivariate normal distributions
# ^ need to install jags directly from 
#    https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Mac%20OS%20X/
#   before installing MVN
library(leaps) # for regsubsets()
library(glmnet)  # for glmnet() (ridge/lasso regression)
library(pls)  # for pcr()
library(rpart)
library(rpart.plot)

select <- dplyr::select # avoid namespace collision with MASS
filter <- dplyr::filter  
```

## 1) Fitting Classification Trees

We will be using the `rpart` library to construct the classification and regression
trees, unlike the 2014 version of Introduction to Statistical Learning.

We first use classification trees to analyze the `Carseats` data set. In these 
data, `Sales` is a continuous variable, and so we begin by recoding it as a
binary variable.  
```{r}
Carseats$High <- ifelse(Carseats$Sales <= 8, "No", "Yes")
```

We will now use the `rpart()` function to fit a classification tree in order to
predict `High` using all variables but `Sales`.
```{r}
set.seed(2)

tree.carseats <- rpart(
  formula = High ~ . - Sales,
  data = Carseats,
  method = "class"
)

tree.carseats$variable.importance

rpart.plot::rpart.plot(tree.carseats)
```
We can see from the plot that there are 11 terminal nodes. Look at the terminal 
node on the far right, we see that it contains all points for which `ShelveLoc`
is `Good` and `Price` is greater than 143. It contains 18% of the points in the
training set, and 86% of those points have a response value of `High` that is
`Yes`.

Let's consider the training error rate by looking at the CP Table:
```{r}
printcp(tree.carseats)
```
`CP` is the complexity parameter (give the complexity
of the tree model), `rel error` is the training error, `xerror` is the cross
validation error, and `xstd` is the standard deviation of the cross validation 
error. Note that each of these columns are scaled by a factor of $1 / \text{Root node error}$
so that $\text{Root node error} \times \text{rel error} = \text{training error}$
and $\text{Root node error} \times \text{x error} = \text{cv error}$.
Note that root node error is the training error rate when no splits have
been taken. See pages 15-16 at
https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf for a guide.

A rule of thumb for selecting the best fit is to select the CP value corresponding
to $\min(\text{nsplit}\colon \text{xerror}\in[\min(\text{xerror}) - \text{xstd}, \min(\text{xerror}) + \text{xstd}])$.
The logic for this is that `xerror` often plateaus as `nsplit` gets large, but 
larger trees have higher variance which increases the risk of overfitting.
To balance these interests, selec the smallest tree within the plateau, defined 
as a 1 standard deviation band around the smallest cross validation error.
This idea is summed up in the cp plot:
```{r}
plotcp(tree.carseats)
```
The lowest (scaled) cross validation error occurs when there are 11 leaf
nodes (10 splits), the dotted line is printed to be 1 standard deviation above
this value.  In this case, it is at a height of $0.53049 + 0.050310$.  In the
language of the above description, this line represents the top of the plateau.
The smallest tree beneath this plateau is the one with 6 nodes (5 splits); it
is that tree that we choose. This tree has a scaled CP value of $0.027439$
and an absolute CP value of $0.41 \times 0.027439$.

To extract the desired tree, call `prune` and pass it the absoluted CP value:
```{r}
fit5 <- prune(tree.carseats, cp = 0.027439 * 0.41)
rpart.plot(fit5)
```
This is the same table as printing with `rpart.plot(tree.carseats)` - the best
model accorinding to the plateau method is printed automatically.

Let's see what the test error rate is like:
```{r}
set.seed(1)

train <- sample(seq(1, nrow(Carseats)), 300)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]

Carseats.tree <- rpart(
  High ~ . - Sales,
  Carseats.train,
  method = "class"
)

printcp(Carseats.tree)
```
We have a 28% error rate with a confusion matrix that 
indicates false negatives occur more often than false
positives. The best CP parameter is the one for `nsplit=4` since the top of the 
plateau is $0.5837 + 0.06041 = 0.64578 > 0.63415$.
```{r}
plotcp(Carseats.tree)
```
