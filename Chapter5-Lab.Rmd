---
title: "Chapter5-Lab"
author: "ZackBarry"
date: "7/6/2019"
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(dplyr)
library(ggplot2)
library(MASS) # for lda()
library(class) # for knn()
library(boot) # for cv.glm() and boot()
library(MVN)  # for checking for multivariate normal distributions
# ^ need to install jags directly from 
#    https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Mac%20OS%20X/
#   before installing MVN

select <- dplyr::select # avoid namespace collision with MASS
filter <- dplyr::filter  
```

# Cross-Validation and the Bootstrap

The methods within this lab all have the same goal: estimate the test mean squared
error before applying the model to the test set in order to further optimize the 
model.  They all involve segmenting the training set into multiple subsets.
Sometimes these subsets are disjoint; other times they are not. 

For all of the examples in this lab, we will be fitting `mpg` as a function of 
`horsepower`.

## The Validation Set Approach

The validation set approach divides the data set into a training set, a validation set,
and a test set.  The training set is used to build the model and the validation set
is used to estimate the MSE before the model is applied to the test set.  This
is helpful to the model creator because they can estimate the test errors of several
different potential models before committing to one for the test set.

We'll be using `ISLR::Auto` for illustrating the validation set approach. 
First, split the data into training and test sets containing 80% and 20% of the 
data.
```{r}
set.seed(1)

data_size <- dim(Auto)[1]

train_size <- ceiling(0.8 * data_size)

train_index = sample(data_size, train_size)
test_index = setdiff(seq(1, data_size), train_index)

train <- Auto[train_index, ]
test <- Auto[test_index, ]

paste("dim train:", dim(train)[1], "| dim test:", dim(test)[1],  
      "| train ratio:", dim(train)[1] / data_size)
```

We repeat the process to split the training set into smaller training set and a 
new validation set, containing 80% and 20% of the training set, respectively.
```{r}
new_train_size <- ceiling(0.8 * train_size)

new_train_index <- sample(train_size, new_train_size)
validation_index <- setdiff(seq(1, train_size), new_train_index)

new_train <- train[new_train_index, ]
validation <- train[validation_index, ]
train <- new_train

paste("dim train:", dim(train)[1], "| dim validation:", dim(validation)[1],
      "| train ratio:", dim(train)[1] / (dim(train)[1] + dim(validation)[1]))
```

Now we can estimate the test MSE for several models by fitting the model to the 
training data set and then calculating the validation set error.  We will select the
model with the lowest validation set MSE to calculate the test MSE for.
```{r}
lm.fit <- lm(mpg ~ horsepower, data = train)
lm.val.error <- mean((predict(lm.fit, validation) - validation$mpg) ^ 2)
print(paste("linear validation MSE:", lm.val.error))

quad.fit <- lm(mpg ~ poly(horsepower, 2), data = train)
quad.val.error <- mean((predict(quad.fit, validation) - validation$mpg) ^ 2)
print(paste("quadratic validation MSE:", quad.val.error))

cube.fit <- lm(mpg ~ poly(horsepower, 3), data = train)
cube.val.error <- mean((predict(cube.fit, validation) - validation$mpg) ^ 2)
print(paste("cubic validation MSE:", cube.val.error))
```

The results indicate to use that the linear regression models with a quadratic
and cubic `horsepower` terms are a better fit than the simple linear model.
However, the validation set MSE is very close between the quadratic and cubic models.
Since these values are likely to change slightly depending on which random subsets
was selected for the training and validation sets, we can not make a definitive decision
between the two models. 
For the sake of choosing a model, though, we will select the 
cubic model as having the potential to perform best on the test set.
Let's see how that selection works out:
```{r}
lm.test.error <- mean((predict(lm.fit, test) - test$mpg) ^ 2)
print(paste("linear test MSE:", lm.test.error))

quad.test.error <- mean((predict(quad.fit, test) - test$mpg) ^ 2)
print(paste("quadratic test MSE:", quad.test.error))

cube.test.error <- mean((predict(cube.fit, test) - test$mpg) ^ 2)
print(paste("cubic test MSE:", cube.test.error))
```

Although the test MSE values are all higher than the validation MSE values,
the models have the same ranked performance -- the linear model performs significantly
worse than the models with a quadratic and cubic term but there is little evidence
that the cubic model is an improvement over the quadratic model.

## Leave-One-Out Cross-Validation

LOOCV estimates the test MSE by fitting the model to each of the $N$
subsets that can be made from the training set by removing one of the $N$ values.
The MSE for each of these $N$ models is $\text{MSE}_i = (\hat{y}_i - y_i)^2$ where
$x_i$ is the point that was removed for the model fit and $\hat{y}_i$ is the 
resulting estimate for the $i$th response variable 
The cross-valiation value is then $\text{CV}_{(N)}=\frac{1}{N}\sum_{i=1}^N \text{MSE}_i$.
This value is expensive to calculate for models which are not solved via 
least squares linear or polynomial regression, but it is much lest biased than
the validation approach since each $\text{MSE}_i$ is calculated using a subset
that is nearly as large as the actual training set.

The LOOCV MSE estimate can be calculated for any generalized linear model
by using `glm()` and `cv.glm()`.  Note that `lm()` and `glm()` return the same results
if the default `family = gaussian` argument is used for `glm()`.

```{r}
train_index <- sample(dim(Auto)[1], ceiling(dim(Auto)[1] * 0.8))

train <- Auto[train_index, ]
test <- Auto[-train_index, ]

for (i in 1:5) {
  loocv.model <- glm(mpg ~ poly(horsepower, degree = i), data = train)
  loocv.error <- cv.glm(data = train, glmfit = loocv.model)$delta[1]
  test.error <- mean((predict(loocv.model, test) - test$mpg) ^ 2)
  print(paste("Poly degree:", i, "| LOOCV est.:", round(loocv.error, 2), "| Test MSE:", round(test.error, 2)))
}
```

As with the validation set approach, we see that the LOOCV estimate does not
match the Test MSE in magnitude, but does a good job matching the Test MSE
in terms of the relative performance of the models.

## k-Fold Cross-Validation

k-fold Cross Validation works similarly to LOOCV.  The data set is divided into
$k$ groups of equal size.  Each of the groups is in turn treated as a validation
set for the model fit to the other $k-1$ groups.  This results in $k$ estimates
of the test MSE so that the cross validation estimate is $\text{CV}_{(k)}=\frac{1}{k}\sum_{i=1}^k\text{MSE}_i$.

This value can also be calculated using `cv.glm()` by setting the `k` argument.
The default `k` value is the 1 so that `cv.glm()` defaults to LOOCV in the 
section above.  A common choice of $k$ is $k=10$.
```{r}
for (i in 1:5) {
  kfold.fit <- glm(mpg ~ poly(horsepower, degree = i), data = train)
  kfold.cv <- cv.glm(train, kfold.fit, K = 10)$delta[1]
  test.error <- mean((predict(kfold.fit, test) - test$mpg) ^ 2)
  print(paste("Poly degree:", i, "| k-Fold est.:", round(kfold.cv, 2), "| Test MSE:", round(test.error, 2)))
}
```

## The Bootstrap

The bootstrap method seeks to estimate the Standard Error of the parameters of
a given model $\{\alpha_i\}_{i\in\mathcal{I}}$.  It does this by sampling $B$ 
different "bootstrap data sets" which are the same size as the training set and are created by sampling the training set with replacement.  The model is then fit
to each of these sets to obtain $B$ estimates of each parameter. Finally the 
standard errors of the bootstrap estimates are calculated by
$$ \text{SE}_B(\hat{\alpha}_i) = \sqrt{\frac{1}{B-1}\sum_{r=1}^B
\left(\hat{\alpha_i}^r - \frac{1}{B}\sum_{r'=1}^B \hat{\alpha_i}^{r'}\right)^2}.$$

We will apply the bootstrap method using the function `boot()` from the `boot`
package.  This function requires 3 arguments - `data`, `statistic`, `R`.
For our purposes, `data` is the training data set and `R` is the number of 
bootstrap data sets to calculate. `statistic` is a "function which when applied
to data returns a vector containing the statistic(s) of interest".  It must take
2 parameters, the first of which accepts the original data and the second of which
accepts a vector of indicies that define the bootstrap data set.

```{r}
boot.statistic <- function(data, subset.indicies) {
  model <- glm(mpg ~ horsepower, data = data, subset = subset.indicies)
  return(coef(model))
}

boot(
  data = train, 
  statistic = boot.statistic, 
  R = 1000
)
```



